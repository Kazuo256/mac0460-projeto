\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{multirow}
\usepackage{hyperref}

\title{MAC0460 - 2º semestre de 2012 \\ Relatório do Projeto}
\author{
  Wilson Kazuo Mizutani\\
  Número USP: 6797230
  \and
  Gustavo Teixeira da Cunha Coelho\\
  Número USP: 6797334
}

\begin{document}
\maketitle

\section{Organização do Projeto}

  Dentro do arquivo compactado que enviamos deve haver uma pasta com os
  seguintes arquivos e diretórios:

  \begin{itemize}
    \item \verb$relatorio.pdf$:
      Esse relatório.
    \item \verb$src/$:
      Pasta com o código fonte do treinador.
    \item \verb$CMakeLists.txt$:
      Arquivo de CMake necessário para compilar o treinador.
    \item \verb$logs/$:
      Pasta com os logs de saída do treinador para os diferentes
      classificadores. Dentro dela haverá arquivos dos tipos:
      \begin{itemize}
        \item \verb$*.out$:
          Saídas da execução do treinador para um classificador específico com
          uma combinação específica de detector/extrator. Nesses arquivos é
          possível encontrar todos os dados relevantes da execução, como tempo
          de execução de cada etapa e o resultado da classificação de cada
          imagem.
        \item \verb$*.err$:
          Saídas de erro da execução do treinador para um classificador
          específico com uma combinação específica de detector/extrator. São
          poucos os logs de erro que contêm algum texto, e estes são apenas
          alertas de que não foi possível salvar o classificador em um arquivo
          (ver seção 5 para mais informações).
        \item \verb$*.test$:
          Arquivo com uma filtragem dos arquivos \verb$*.out$ de todas as
          combinações detector/extrator de um mesmo classificador, contendo
          apenas o desempenho (acertos/total) dele.
        \item \verb$*.times$:
          Arquivo com uma filtragem dos arquivos \verb$*.out$ de todas as
          combinações detector/extrator de um mesmo classificador, contendo
          apenas o tempo de execução das etapas de detecção/extração de
          descritores e histogramas dele (ver seção 5 para mais informações).
        \item \verb$confusion.awk$ e \verb$confusion.sh$:
          Um par de scripts que juntos analisam os logs e geram uma saída com as
          matrizes de confusão das melhores combinações detector/extrator de
          cada um dos classificadores (ver seção 6 para mais detalhes).
        \item \verb$confusion.coelho$:
          A saída resultante da exucução do script \verb$confusion.sh$.
      \end{itemize}
    \item \verb$runTest.sh$, \verb$runTestBattery.sh$ e \verb$runTests.sh$:
      Scripts para execução automátizada em diferentes níveis do treinador. O
      modo de usá-los é explicado na seção 2.
    \item \verb$training.set$ e \verb$test.set$:
      Arquivos que listam as imagens de treinamento e de teste, respectivamente.
      Fizemos ele para podermos "iterar" pelas imagens.
  \end{itemize}

\section{Compilando e rodando o projeto}

  Para compilar...

  \subsection{Execução do treinador}

    Antes de mais nada, é precisa extrair as imagens de treinamento e de teste
    nas pastas \verb$training_set$ e \verb$test_set$, respectivamente, tomando o
    cuidado de renomear a pasta \verb$test_Set/piramides$ para
    \verb$test_set/piramide$ (como está no conjunto de treinamento), pois o
    treinador usa os nomes dessas pastas para identificar as classes. O
    importante é que os caminhos fiquem extamente como estão nos arquivos
    \verb$training.set$ e \verb$test.set$.

    Para rodar o programa, recomendamos o uso dos scripts bash que entregamos
    junto com o projeto. Segue a explicação de cada um deles:

    \begin{itemize}
      \item \verb| ./runTest.sh <classificador> <detector> <extrator> |

      Onde:
      \begin{itemize}
        \item[<classificador>]
          Deve ser "\verb$bayes_classifier$", "\verb$knn_classifier$", \\
          "\verb$dtree_classifier$" ou "\verb$rtrees_classifier$". Sem as aspas.
        \item[<detector>]
          Deve ser um dos especificados \href{
          http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_feature_detectors.html#featuredetector-create
          }{nesse link}. Incluindo o uso dos adaptadores.
        \item[<extrator>]
          Deve ser um dos especificados \href{
          http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_descriptor_extractors.html#descriptorextractor-create
          }{nesse link}.
      \end{itemize}
      Esse script roda o treinador para o classificador especificado, com a
      combinação de detector (com ou sem adaptador) e extrator especificada.

      Como será visto na seção 3, nem todas as combinações de detectores,
      adaptadores e extratores com classificadores funcionam. Para saber quais
      devem funcionar, é só ver na seção 3.

      \item \verb| ./runTestBattery.sh <classificador>|

      Onde:
      \begin{itemize}
        \item[<classificador>]
          Deve ser "\verb$bayes_classifier$", "\verb$knn_classifier$", \\
          "\verb$dtree_classifier$" ou "\verb$rtrees_classifier$". Sem as aspas.
      \end{itemize}
      Esse script roda uma bateria de testes completa para o classificador
      passado, usando todas as combinações de detectores (com e sem adaptador)
      e extratores que sabemos que funcionam (mencionados na seção 3). Na
      verdade, haverá um dos testes que resultará em falha de segmentação (por
      culpa do OpenCV), a combinação ORB-SIFT, como será explicado na seção 3.


      \item \verb| ./runTests.sh|

      Esse script roda todos os testes que usamos no projeto, combinando todos
      os classificadores escolhidos com os possíveis detectores, adaptadores e
      extratores. A bateria de testes de cada classificador é executada em um
      processo paralelo, portanto executar esse script consome bastantes
      recursos da máquina (embora seja bem simples alterá-lo para que tudo seja
      sequencial).

      Devido ao mesmo motivo do script anterior, haverá quatro testes que
      resultarão em falha de segmentação.
    \end{itemize}

\section{Decisões de projeto}

  Usamos os Detectores e Extratores que não apresentaram problema de execução,
  como incompatibilidade de formatos das matrizes, ou falhas de segmentação
  dentro da própria biblioteca do OpenCV. Os classificadores escolhemos mais
  por preferência, mas também acabamos descartando alguns pelos mesmo motivos.  

  Isso nos deixou com as seguintes possibilidades:

  \begin{itemize}
    \item Classificadores:
    \begin{itemize}
      \item[-] Normal Bayes
      \item[-] K-Nearest Neighbours
      \item[-] Decisions Tree
      \item[-] Random Trees
    \end{itemize}
    
    \item Detectores e Extratores
      \begin{itemize}
        \item Detectores usados:
          \begin{itemize}
            \item STAR
            \item SURF
            \item SIFT
            \item ORB\footnotemark
          \end{itemize}
  
        \item Adaptadores:
          \begin{itemize} 
            \item Pyramid (usamos cada detector com e sem esse adaptador)
          \end{itemize}
  
        \item Extratores usados:
          \begin{itemize} 
            \item SURF
            \item SIFT
          \end{itemize}
  
      \end{itemize}
  
  \end{itemize}
  
  \footnotetext{
    A combinação específica de detector/extrator ORB+SIFT causava falha de
    segmentação.
  }

\section{Escolha de Parâmetros}

  O único parâmetro que necessita escolha é o número de clusters do treinador de
  Bag of Words (no caso, o \verb$BOWKMeansTrainer$). Originalmente tentamos com
  1000 clusters, mas a execução demorava mais de 2 horas. Mudamos para 100 e
  fomos aumentando meio arbitrariamente. Quando notamos que os resultados saiam
  melhores e mais rápidos com 256 clusters do que com 400, optamos por 256
  clusters.

\section{Otimizações utilizadas}

  Usamos o recurso de salvar classificadores em arquivos sempre que a respectiva
  implementação permitia, para evitar que eles fossem treinados a cada vez que
  rodássemos o programa. O único classificador que não tem essa funcionalidade
  implementada é o K-Nearest Neighbors.

  Também salvamos em arquivos os vocabulários gerados para cada combinação
  detector/extrator. Eles eram o maior gargalo das execuções (alguns passavam de
  20 minutos). Como esses arquivos podiam ser aproveitados de um classificador
  para outro, ganhamos bastante tempo na hora de rodar as baterias de testes.

  Graças a isso, a parte temporalmente relevante das computações reduziram-se ao
  tempo de detecção de KeyPoints/descritores e ao de extração de histogramas,
  que só dependem da combinação detector/extrator e não do classificador usado.

\section{Resultados obtidos e conclusões}

  Abaixo segue uma tabela contendo o erro máximo de um classificador dado os
  resultados dos testes. Esse erro é calculado a partir da fórmula:

  \begin{center}
    $ Erro_T(g)+Z_n * \sqrt{\frac{Erro_T(g)(1 - Erro_T(g))}{n}} $
  \end{center}

  Onde $n$ é o número de amostras de teste, $Erro_T$ é a porcentagem de erro do
  classificador e $Z_n$ é uma constante indicando o intervalo de confiança.
  Neste caso usamos um intervalo de confiança de 90\% (e consequentemente um
  $Z_n$ de 1.64), mas como tal intervalo denota o intervalo entre o erro máximo
  e o erro mínimo, consideramos apenas o erro máximo, que então é tido com 95\%
  de confiança.

  \vspace{15pt}
  \hspace{-25pt}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
     & Bayes & kNN & Random Trees & Decision Tree \\
    \hline
        STAR-SURF & 48,38\% & 77,35\% & 72,52\% & 83,57\% \\
        SURF-SURF & 23,31\% & 46,56\% & 35,29\% & 72,52\% \\
        SIFT-SURF & 53,77\% & 65,88\% & 53,77\% & 78,93\%\\
        ORB-SURF & 25,38\% & 48,38\% & 46,56\% & 70,88\% \\
        PyramidSTAR-SURF & 50,19\% & 75,75\% & 62,48\% & 77,35\% \\
        PyramidSURF-SURF & 29,41\% & 42,87\% & 42,87\% & 80,49\% \\
        PyramidSIFT-SURF & 42,87\% & 57,29\% & 46,56\% & 60,77\% \\
        PyramidORB-SURF & 19,08\% & 31,39\% & 29,41\% & 65,88\% \\
        STAR-SIFT & 21,22\% & 27,41\% & 27,41\% & 55,54\% \\
        SURF-SIFT & 27,41\% & 27,41\% & 31,39\% & 55,54\% \\
        SIFT-SIFT & 31,39\% & 50,19\% & 44,72\% & 67,56\% \\
        PyramidSTAR-SIFT & 12,35\% & 14,66\% & 23,31\% & 46,56\% \\
        PyramidSURF-SIFT & 27,41\% & 31,39\% & 27,41\% & 46,56\% \\
        PyramidSIFT-SIFT & 12,35\% & 39,11\% & 35,29\% & 64,19\% \\
        PyramidORB-SIFT & 23,31\% & 29,41\% & 21,29\% & 44,72\% \\
        
    \hline
  \end{tabular}
  \bigskip

  É fácil deduzir da tabela que o classificador Bayesiano é melhor que os outros
  três. De fato, ele foi melhor em todas as combinações de detector/extrator,
  exceto contra o Classificador Random Trees usando a combinação
  \textit{PyramidORB-SIFT}.

  Abaixo segue uma tabela do tempo total tomado para detecção de KeyPoints,
  extração de descritores e extração de histogramas. O tempo para criar o
  vocabulário e treino do classificador é ignorado aqui pois, como já
  mencionado, ambos o classificador e o vocabulário podem ser salvos em arquivos
  para uso posterior.

  \vspace{15pt}
  \hspace{65pt}
  \begin{tabular}{|c|c|c|}
    \hline
    Feature &  \multicolumn{2}{|c|}{Descriptor Extractor} \\
    \cline{2-3}
     Detectors & SURF & SIFT \\
    \hline
    STAR & 11,25 & 57 \\
    SURF & 108,5 & 524,5 \\
    SIFT  & 26 & 47,25 \\
    ORB  & 50 & X \\
    PyramidSTAR & 13,75 & 66,5 \\
    PyramidSURF & 172,75 & 1396 \\
    PyramidSIFT & 37 & 111,25 \\
    PyramidORB & 222,5 & 1634,75 \\
    \hline
  \end{tabular}
  \vspace{15pt}

  Abaixo seguem as matrizes de confusão de cada um dos melhores resultados de
  combinação detector/extrator para cada um dos classificadores usados.

  \vspace{15pt}
  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador Normal Bayes} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidSIFT + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 11 & 0 & 0 & 0 & 0 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 1 & 8 & 0 & 0 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 0 & 1 & 0 & 8 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 0 & 0 & 0 & 0 & 9 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 0 & 0 & 0 & 0 & 0 & 10\\
    \hline
  \end{tabular}
  \bigskip
  
  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador kNN} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidSTAR + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 11 & 0 & 0 & 0 & 0 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 2 & 8 & 0 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 0 & 2 & 0 & 7 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 0 & 0 & 0 & 0 & 9 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 0 & 0 & 0 & 0 & 0 & 10\\
    \hline
  \end{tabular}
  \bigskip

  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador Random Trees} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidORB + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 10 & 1 & 0 & 0 & 0 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 2 & 8 & 0 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 2 & 2 & 0 & 6 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 1 & 0 & 0 & 0 & 8 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 0 & 0 & 0 & 0 & 0 & 10\\
    \hline
  \end{tabular}
  \bigskip

  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador Decision Tree} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidORB + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 9 & 0 & 0 & 1 & 1 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 9 & 0 & 0 & 1 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 3 & 0 & 0 & 6 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 3 & 0 & 0 & 0 & 6 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 1 & 0 & 0 & 0 & 0 & 9\\
    \hline
  \end{tabular}
  \vspace{15pt}

  Uma observação que podemos fazer aqui é que, enquanto os classificadores
  Bayesiano, K-Nearest Neighbors e Random Trees obtiveram matrizes de confusão
  boas (o Bayesiano ainda sobressaindo-se), o de Decision Tree teve uma
  particularmente pior. Ele errou todas as classificações de imagens da Muralha
  da China. Provavelmente essa combinação específica de detector/extrator só foi
  a melhor dele por acaso, e mesmo assim, como é possível observar na primeira
  tabela, ainda ficou com um erro máximo de 44,72\%.

\end{document}

