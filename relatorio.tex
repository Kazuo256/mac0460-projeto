\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{multirow}
\usepackage{hyperref}

\title{MAC0460 - 2º semestre de 2012 \\ Relatório do Projeto}
\author{
  Wilson Kazuo Mizutani\\
  Número USP: 6797230
  \and
  Gustavo Teixeira da Cunha Coelho\\
  Número USP: 6797334
}

\begin{document}
\maketitle

\section{Organização do Projeto}

  Dentro do arquivo compactado que enviamos deve haver uma pasta (a qual nos
  referiremos como sendo a pasta raiz do projeto) com os seguintes arquivos e
  diretórios:

  \begin{itemize}
    \item \verb$relatorio.pdf$:
      Esse relatório.
    \item \verb$src/$:
      Pasta com o código fonte do treinador.
    \item \verb$CMakeLists.txt$:
      Arquivo de CMake necessário para compilar o treinador.
    \item \verb$logs/$:
      Pasta com os logs de saída do treinador para os diferentes
      classificadores. Dentro dela haverá arquivos dos tipos:
      \begin{itemize}
        \item \verb$*.out$:
          Saídas da execução do treinador para um classificador específico com
          uma combinação específica de detector/extrator. Nesses arquivos é
          possível encontrar todos os dados relevantes da execução, como tempo
          de execução de cada etapa e o resultado da classificação de cada
          imagem.
        \item \verb$*.err$:
          Saídas de erro da execução do treinador para um classificador
          específico com uma combinação específica de detector/extrator. São
          poucos os logs de erro que contêm algum texto, e estes são apenas
          alertas de que não foi possível salvar o classificador em um arquivo
          (ver seção 5 para mais informações).
        \item \verb$*.test$:
          Arquivo com uma filtragem dos arquivos \verb$*.out$ de todas as
          combinações detector/extrator de um mesmo classificador, contendo
          apenas o desempenho (acertos/total) dele.
        \item \verb$*.times$:
          Arquivo com uma filtragem dos arquivos \verb$*.out$ de todas as
          combinações detector/extrator de um mesmo classificador, contendo
          apenas o tempo de execução das etapas de detecção/extração de
          descritores e histogramas dele (ver seção 5 para mais informações).
        \item \verb$confusion.awk$ e \verb$confusion.sh$:
          Um par de scripts que juntos analisam os logs e geram uma saída com as
          matrizes de confusão das melhores combinações detector/extrator de
          cada um dos classificadores (ver seção 6 para mais detalhes).
        \item \verb$confusion.coelho$:
          A saída resultante da exucução do script \verb$confusion.sh$.
      \end{itemize}
    \item \verb$runTest.sh$, \verb$runTestBattery.sh$ e \verb$runTests.sh$:
      Scripts para execução automatizada em diferentes níveis do treinador. O
      modo de usá-los é explicado na seção 2.
    \item \verb$training.set$ e \verb$test.set$:
      Arquivos que listam as imagens de treinamento e de teste, respectivamente.
      Fizemos ele para podermos "iterar" pelas imagens.
  \end{itemize}

\section{Compilando e rodando o projeto}

  \subsection{Compilação do projeto}

    Usamos \href{http://cmake.org/}{CMake} (assim como o próprio OpenCV) para
    compilar nosso projeto. É preciso rodá-lo antes de poder compilar o programa
    em si. Em GNU/Linux, isso é feito com o comando:

    \begin{center}
      \verb1$ cmake -DOpenCV_DIR=<diretorio_do_opencv> .1
    \end{center}

    Onde <diretorio\_do\_opencv> é ou a pasta na qual o OpenCV foi compilado, em
    geral nomeada como \verb$build$ e que deve conter o arquivo
    \verb$OpenCVConfig.cmake$; ou a pasta onde fica o pacote instalado do
    OpenCV, que por padrão fica em \verb$/usr/local$. Porém testamos apenas com
    o OpenCV compilado (última versão estável), então não podemos garantir que o
    segundo método funcione.

    Além disso, foi testado o projeto com Windows usando Visual Studio, usando o
    CMake para gerar um arquivo de Projeto do Visual Studio. Para rodar os
    executáveis, é necessário adicionar os arquivos DLL ao Path do windows.
    Esses arquivos são encontrados em \verb$/opencv/bin$, com as subpastas
    \verb$Release$ e \verb$Debug$, dependendo de com qual configuração de
    solução do Visual Studio o OpenCV foi compilado.

    Em GNU/Linux, essas instruções irão gerar um arquivo de Makefile que poderá
    ser rodado usando o comando

    \begin{center}
      \verb1$ make1
    \end{center}

    A partir da pasta raiz do nosso projeto. Com isso, serão gerados quatro
    executáveis, um para cada classificador: \verb$bayes_classifier$,
    \verb$knn_classifier$, \verb$dtree_classifier$ e \verb$rtrees_classifier$.

  \subsection{Execução do treinador}

    Antes de mais nada, é preciso extrair as imagens de treinamento e de teste
    nas pastas \verb$training_set$ e \verb$test_set$, respectivamente, tomando o
    cuidado de renomear a pasta \verb$test_set/piramides$ para
    \verb$test_set/piramide$ (como está no conjunto de treinamento), pois o
    treinador usa os nomes dessas pastas para identificar as classes. O
    importante é que os caminhos fiquem extamente como estão nos arquivos
    \verb$training.set$ e \verb$test.set$.

    É possível rodar o treinador para uma combinação específica de
    classificador, detector e extrator, e também é possível rodar baterias
    completas de testes. Segue uma explicação de cada uma dessas possibilidads:

    \begin{itemize}
      \item \verb| ./<classificador> <detector> <extrator> |

      Onde:
      \begin{itemize}
        \item[<classificador>]
          Deve ser "\verb$bayes_classifier$", "\verb$knn_classifier$", \\
          "\verb$dtree_classifier$" ou "\verb$rtrees_classifier$". Sem as aspas.
        \item[<detector>]
          Deve ser um dos detectores especificados \href{
          http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_feature_detectors.html#featuredetector-create
          }{nesse link}. O uso dos adaptadores também é válido.
        \item[<extrator>]
          Deve ser um dos extratores especificados \href{
          http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_descriptor_extractors.html#descriptorextractor-create
          }{nesse link}.
      \end{itemize}
      Executa o treinador usando o classificador especificado, com a combinação
      de detector (com ou sem adaptador) e extrator especificada. Os resultados
      e detalhes das diversas etapas do processo de aprendizagem são impressas
      na saída padrão.

      Como será visto na seção 3, nem todas as combinações de detectores,
      adaptadores e extratores com classificadores funcionam. Para saber quais
      devem funcionar, é só ver na seção 3.

      \item \verb| ./runTestBattery.sh <classificador>|

      Onde:
      \begin{itemize}
        \item[<classificador>]
          Deve ser "\verb$bayes_classifier$", "\verb$knn_classifier$", \\
          "\verb$dtree_classifier$" ou "\verb$rtrees_classifier$". Sem as aspas.
      \end{itemize}
      Esse script roda uma bateria de testes completa para o classificador
      passado, usando todas as combinações de detectores (com e sem adaptador)
      e extratores que sabemos que funcionam (mencionados na seção 3). Na
      verdade, haverá um dos testes que resultará em falha de segmentação (por
      culpa do OpenCV), a combinação ORB-SIFT, como será explicado na seção 3.

      A saída é direcionada para os respectivos arquivos na pasta \verb$logs$.

      \item \verb| ./runTests.sh|

      Esse script roda todos os testes que usamos no projeto, combinando todos
      os classificadores escolhidos com os possíveis detectores, adaptadores e
      extratores. A bateria de testes de cada classificador é executada em um
      processo paralelo, portanto executar esse script consome bastantes
      recursos da máquina (embora seja bem simples alterá-lo para que tudo seja
      sequencial).

      Devido ao mesmo motivo do script anterior, haverá quatro testes que
      resultarão em falha de segmentação - um para cada classificador.

      A saída também é direcionada para os respectivos arquivos na pasta
      \verb$logs$.
    \end{itemize}

\section{Decisões de projeto}

  Usamos os Detectores e Extratores que não apresentaram problema de execução,
  como incompatibilidade de formatos das matrizes, ou falhas de segmentação
  dentro da própria biblioteca do OpenCV, embora uma certa combinação específica
  não funcione. Os classificadores foram escolhidos mais por preferência, mas
  também acabamos descartando alguns pelos mesmo motivos.  

  Isso nos deixou com as seguintes possibilidades:

  \begin{itemize}
    \item Classificadores:
    \begin{itemize}
      \item[-] Normal Bayes
      \item[-] K-Nearest Neighbours
      \item[-] Decisions Tree
      \item[-] Random Trees
    \end{itemize}
    
    \item Detectores e Extratores
      \begin{itemize}
        \item Detectores usados:
          \begin{itemize}
            \item STAR
            \item SURF
            \item SIFT
            \item ORB\footnotemark
              \footnotetext{
                A combinação específica de detector/extrator ORB+SIFT causava
                falha de segmentação. A PyramidORB+SIFT não teve problemas.
              }
          \end{itemize}

  
        \item Adaptadores:
          \begin{itemize} 
            \item Pyramid (usamos cada detector com e sem esse adaptador)
          \end{itemize}
  
        \item Extratores usados:
          \begin{itemize} 
            \item SURF
            \item SIFT
          \end{itemize}
  
      \end{itemize}
  
  \end{itemize}
  
\section{Escolha de Parâmetros}

  O único parâmetro que necessita escolha é o número de clusters do treinador de
  Bag of Words (no caso, o \verb$BOWKMeansTrainer$). Originalmente tentamos com
  1000 clusters, mas a execução demorava mais de 2 horas. Mudamos para 100 e
  fomos aumentando meio arbitrariamente. Quando notamos que os resultados saiam
  melhores e mais rápidos com 256 clusters do que com 400, optamos por 256
  clusters.

\section{Otimizações utilizadas}

  Usamos o recurso de salvar classificadores em arquivos sempre que a respectiva
  implementação permitia, para evitar que eles fossem treinados a cada vez que
  rodássemos o programa. O único classificador que não tem essa funcionalidade
  implementada é o K-Nearest Neighbors.

  Também salvamos em arquivos os vocabulários gerados para cada combinação
  detector/extrator. Eles eram o maior gargalo das execuções (alguns passavam de
  20 minutos). Como esses arquivos podiam ser aproveitados de um classificador
  para outro, ganhamos bastante tempo na hora de rodar as baterias de testes.

  Graças a isso, a parte temporalmente relevante das computações reduziram-se ao
  tempo de detecção de KeyPoints/descritores e ao de extração de histogramas,
  que só dependem da combinação detector/extrator e não do classificador usado.

\section{Resultados obtidos e conclusões}

  Abaixo segue uma tabela contendo o erro máximo de um classificador dado os
  resultados dos testes. Esse erro é calculado a partir da fórmula:

  \begin{center}
    $ Erro_T(g)+Z_n * \sqrt{\frac{Erro_T(g)(1 - Erro_T(g))}{n}} $
  \end{center}

  Onde $n$ é o número de amostras de teste, $Erro_T$ é a porcentagem de erro do
  classificador e $Z_n$ é uma constante indicando o intervalo de confiança.
  Neste caso usamos um intervalo de confiança de 90\% (e consequentemente um
  $Z_n$ de 1.64), mas como tal intervalo denota o intervalo entre o erro máximo
  e o erro mínimo, consideramos apenas o erro máximo, que então é tido com 95\%
  de confiança.

  \vspace{10pt}
  \hspace{-25pt}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
     & Bayes & kNN & Random Trees & Decision Tree \\
    \hline
        STAR-SURF & 48,38\% & 77,35\% & 72,52\% & 83,57\% \\
        SURF-SURF & 23,31\% & 46,56\% & 35,29\% & 72,52\% \\
        SIFT-SURF & 53,77\% & 65,88\% & 53,77\% & 78,93\%\\
        ORB-SURF & 25,38\% & 48,38\% & 46,56\% & 70,88\% \\
        PyramidSTAR-SURF & 50,19\% & 75,75\% & 62,48\% & 77,35\% \\
        PyramidSURF-SURF & 29,41\% & 42,87\% & 42,87\% & 80,49\% \\
        PyramidSIFT-SURF & 42,87\% & 57,29\% & 46,56\% & 60,77\% \\
        PyramidORB-SURF & 19,08\% & 31,39\% & 29,41\% & 65,88\% \\
        STAR-SIFT & 21,22\% & 27,41\% & 27,41\% & 55,54\% \\
        SURF-SIFT & 27,41\% & 27,41\% & 31,39\% & 55,54\% \\
        SIFT-SIFT & 31,39\% & 50,19\% & 44,72\% & 67,56\% \\
        PyramidSTAR-SIFT & 12,35\% & 14,66\% & 23,31\% & 46,56\% \\
        PyramidSURF-SIFT & 27,41\% & 31,39\% & 27,41\% & 46,56\% \\
        PyramidSIFT-SIFT & 12,35\% & 39,11\% & 35,29\% & 64,19\% \\
        PyramidORB-SIFT & 23,31\% & 29,41\% & 21,29\% & 44,72\% \\
        
    \hline
  \end{tabular}
  \bigskip

  É fácil deduzir da tabela que o classificador Bayesiano é melhor que os outros
  três. De fato, ele foi melhor em todas as combinações de detector/extrator,
  exceto contra o Classificador Random Trees usando a combinação
  \textit{PyramidORB-SIFT}.

  Abaixo segue uma tabela do tempo total, em segundos, tomado para detecção de
  KeyPoints, extração de descritores e extração de histogramas. O tempo para
  criar o vocabulário e treino do classificador é ignorado aqui pois, como já
  mencionado, ambos o classificador e o vocabulário podem ser salvos em arquivos
  para uso posterior.

  \vspace{10pt}
  \hspace{65pt}
  \begin{tabular}{|c|c|c|}
    \hline
    Feature &  \multicolumn{2}{|c|}{Descriptor Extractor} \\
    \cline{2-3}
     Detectors & SURF & SIFT \\
    \hline
    STAR & 11,25 & 57 \\
    SURF & 108,5 & 524,5 \\
    SIFT  & 26 & 47,25 \\
    ORB  & 50 & X \\
    PyramidSTAR & 13,75 & 66,5 \\
    PyramidSURF & 172,75 & 1396 \\
    PyramidSIFT & 37 & 111,25 \\
    PyramidORB & 222,5 & 1634,75 \\
    \hline
  \end{tabular}
  \vspace{10pt}

  Abaixo seguem as matrizes de confusão de cada um dos melhores resultados de
  combinação detector/extrator para cada um dos classificadores usados:

  \vspace{10pt}
  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador Normal Bayes} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidSIFT + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 11 & 0 & 0 & 0 & 0 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 1 & 8 & 0 & 0 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 0 & 1 & 0 & 8 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 0 & 0 & 0 & 0 & 9 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 0 & 0 & 0 & 0 & 0 & 10\\
    \hline
  \end{tabular}
  \bigskip
  
  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador kNN} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidSTAR + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 11 & 0 & 0 & 0 & 0 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 2 & 8 & 0 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 0 & 2 & 0 & 7 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 0 & 0 & 0 & 0 & 9 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 0 & 0 & 0 & 0 & 0 & 10\\
    \hline
  \end{tabular}
  \bigskip

  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador Random Trees} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidORB + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 10 & 1 & 0 & 0 & 0 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 2 & 8 & 0 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 2 & 2 & 0 & 6 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 1 & 0 & 0 & 0 & 8 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 0 & 0 & 0 & 0 & 0 & 10\\
    \hline
  \end{tabular}
  \bigskip

  \hspace{-75pt}
  \begin{tabular}{|cc|c|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{Classificador Decision Tree} &
    \multicolumn{6}{|c|}{Respostas do Classificador} \\
    \cline{3-8}
    \multicolumn{2}{|c|}{com PyramidORB + SIFT}
    & Cristo & Muralha & Panteão & Pirâmides & Taj Mahal & Torre Eiffel \\
    \hline
    \multirow{6}{*}{Classes Reais}
    & \multicolumn{1}{|c|}{Cristo} & 9 & 0 & 0 & 1 & 1 & 0 \\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Muralha} & 9 & 0 & 0 & 1 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Panteão} & 0 & 0 & 8 & 0 & 0 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Pirâmides} & 3 & 0 & 0 & 6 & 1 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Taj Mahal} & 3 & 0 & 0 & 0 & 6 & 0\\
    \cline{2-8}
    & \multicolumn{1}{|c|}{Torre Eiffel} & 1 & 0 & 0 & 0 & 0 & 9\\
    \hline
  \end{tabular}
  \vspace{10pt}

  Uma observação que podemos fazer aqui é que, enquanto os classificadores
  Bayesiano, K-Nearest Neighbors e Random Trees obtiveram matrizes de confusão
  boas (o Bayesiano ainda se sobressaindo), o de Decision Tree teve uma
  particularmente pior. Ele errou todas as classificações de imagens da Muralha
  da China. Provavelmente essa combinação específica de detector/extrator só foi
  a melhor dele por acaso, e mesmo assim, como é possível observar na primeira
  tabela, ainda ficou com um erro máximo de 44,72\%.

\end{document}

